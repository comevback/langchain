import time
import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_openai import AzureChatOpenAI  # ä»ä¿ç•™ Azure ä½œä¸ºé—®ç­”æ¨¡å‹
from langchain_huggingface import HuggingFaceEmbeddings


# === 1ï¸âƒ£ åŠ è½½ç¯å¢ƒå˜é‡ ===
load_dotenv()
index_file = "faiss_index_local"

# === 2ï¸âƒ£ Azure èŠå¤©æ¨¡å‹é…ç½®ï¼ˆç”¨äºé—®ç­”é˜¶æ®µï¼‰ ===
AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_CHAT_DEPLOYMENT = os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT")
OPENAI_API_VERSION = os.getenv("OPENAI_API_VERSION")

# === 3ï¸âƒ£ åŠ è½½ PDF æ–‡æ¡£ ===
loader = PyPDFLoader("./Pythonæ ¸å¿ƒç¼–ç¨‹ (Wesley Chun) (Z-Library).pdf")
docs = loader.load()

# === 4ï¸âƒ£ æ–‡æœ¬åˆ‡å— ===
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
chunks = splitter.split_documents(docs)

# === 5ï¸âƒ£ åˆå§‹åŒ–æœ¬åœ° Embedding æ¨¡å‹ ===
# å¯é€‰æ¨¡å‹ï¼š
# "intfloat/multilingual-e5-base"  â†’ æ¨è (ä¸­è‹±åŒè¯­)
# "BAAI/bge-base-zh"               â†’ ä¸­æ–‡è¡¨ç°æ›´å¼ºï¼ˆç¨æ…¢ï¼‰
embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-base")

# === 6ï¸âƒ£ æ£€æŸ¥å·²æœ‰ç´¢å¼•ï¼Œå‡†å¤‡æ–­ç‚¹ç»­è·‘ ===
all_texts, all_metas = [], []
processed_count = 0
vectorstore = None

if os.path.exists(index_file):
    try:
        vectorstore = FAISS.load_local(
            index_file, embeddings, allow_dangerous_deserialization=True
        )
        processed_count = len(vectorstore.docstore._dict)
        print(f"âœ… å·²å¤„ç† {processed_count} ä¸ª chunkï¼Œè·³è¿‡è¿™äº›ï¼Œç»§ç»­å¤„ç†å‰©ä½™éƒ¨åˆ†ã€‚")
        all_texts = [chunks[i].page_content for i in range(processed_count)]
        all_metas = [chunks[i].metadata for i in range(processed_count)]
    except Exception as e:
        print(f"âš ï¸ åŠ è½½ç°æœ‰ç´¢å¼•å¤±è´¥ï¼Œå°†é‡æ–°åˆ›å»º: {e}")

# === 7ï¸âƒ£ æ‰¹é‡ç”Ÿæˆ Embedding å¹¶ä¿å­˜ç´¢å¼• ===
batch_size = 10
chunks_to_process = chunks[processed_count:]
print(f"ğŸ‘‰ å…±æœ‰ {len(chunks_to_process)} ä¸ªå¾…å¤„ç† chunkã€‚")

for i in range(0, len(chunks_to_process), batch_size):
    batch = chunks_to_process[i:i + batch_size]
    texts = [doc.page_content for doc in batch]
    metas = [doc.metadata for doc in batch]
    all_texts.extend(texts)
    all_metas.extend(metas)

    print(f"ğŸ”¹ å¤„ç†è¿›åº¦: {processed_count + i + len(batch)}/{len(chunks)}")

    # æœ¬åœ° embedding ä¸éœ€è¦é‡è¯•ï¼Œä¹Ÿä¸ä¼š 429
    embeddings.embed_documents(texts)

    # æ¯æ‰¹ä¿å­˜ä¸€æ¬¡è¿›åº¦
    try:
        vectorstore = FAISS.from_texts(
            all_texts, embeddings, metadatas=all_metas)
        vectorstore.save_local(index_file)
        print(f"ğŸ’¾ å·²ä¿å­˜è¿›åº¦åˆ° {index_file}")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜æœ¬åœ°è¿›åº¦å¤±è´¥: {e}")

# === 8ï¸âƒ£ å‘é‡åŒ–å®Œæˆ ===
print("âœ… æœ¬åœ°å‘é‡åŒ–å®Œæˆï¼Œæ­£åœ¨æ„å»ºé—®ç­”ç³»ç»Ÿ...")

# === 9ï¸âƒ£ æ„å»º QA é“¾ ===
llm = AzureChatOpenAI(
    azure_deployment=AZURE_CHAT_DEPLOYMENT,
    openai_api_version=OPENAI_API_VERSION,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_key=AZURE_OPENAI_API_KEY
)

qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    return_source_documents=True
)

# === ğŸ”Ÿ æé—® ===
query = "ã€ŠPythonæ ¸å¿ƒç¼–ç¨‹ã€‹ä¸­ç¬¬8ç« æ‰©å±•Pythonå…·ä½“è®²äº†ä»€ä¹ˆï¼Œæˆ‘ä¼šå­¦åˆ°ä»€ä¹ˆï¼Œå­¦ä¼šä¹‹åæˆ‘èƒ½ç”¨è¿™ä¸ªå‘æŒ¥ä»€ä¹ˆä½œç”¨ã€‚"
print("\nQ:", query)
result = qa.invoke(query)
print("\nA:", result['result'])
print("\nğŸ“– æ£€ç´¢ç‰‡æ®µï¼š")
for i, doc in enumerate(result['source_documents']):
    print(f"\n--- ç‰‡æ®µ{i+1} ---\n{doc.page_content[:500]}...")
