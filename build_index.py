import os
from tqdm import tqdm  # âœ… è¿›åº¦æ¡
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# è·¯å¾„
DATA_DIR = "data"
INDEX_DIR = "faiss_index"

# 1ï¸âƒ£ åŠ è½½ embedding æ¨¡å‹ï¼ˆå¤šè¯­è¨€ï¼Œé€‚åˆä¸­è‹±æ–‡ï¼‰
embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-base")

# 2ï¸âƒ£ åˆ‡åˆ†å™¨
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

# 3ï¸âƒ£ é¢„æ‰«ææ‰€æœ‰ PDFï¼Œç»Ÿè®¡æ€» chunk æ•°ï¼ˆç”¨äºè¿›åº¦æ¡ï¼‰
file_list = [f for f in os.listdir(DATA_DIR) if f.lower().endswith(".pdf")]
all_docs_info = []  # [(filename, chunks)]
total_chunks = 0
for filename in file_list:
    filepath = os.path.join(DATA_DIR, filename)
    loader = PyPDFLoader(filepath)
    docs = loader.load()
    chunks = splitter.split_documents(docs)
    all_docs_info.append((filename, chunks))
    total_chunks += len(chunks)

if total_chunks == 0:
    print("âš ï¸ data/ ç›®å½•ä¸‹æ²¡æœ‰å¯å¤„ç†çš„ PDFï¼Œæˆ–åˆ‡åˆ†åä¸ºç©ºã€‚")
    exit(0)

print(f"ğŸ“¦ å°†å¤„ç† {len(file_list)} ä¸ª PDFï¼Œå…± {total_chunks} ä¸ª chunkã€‚")

# 4ï¸âƒ£ å¢é‡æ„å»ºç´¢å¼•ï¼ˆåˆ†æ‰¹ + è¿›åº¦æ¡ + å‘¨æœŸæ€§ä¿å­˜ï¼‰
batch_size = 64           # æ¯æ‰¹å¤„ç†çš„ chunk æ•°é‡
save_every_n_batches = 5  # æ¯ N æ‰¹ä¿å­˜ä¸€æ¬¡

vectorstore = None
processed = 0
batch_texts, batch_metas = [], []

with tqdm(total=total_chunks, desc="Embedding & Indexing", unit="chunk") as pbar:
    for filename, chunks in all_docs_info:
        print(f"ğŸ“˜ æ­£åœ¨å¤„ç†: {filename}ï¼ˆ{len(chunks)} æ®µï¼‰")

        for doc in chunks:
            batch_texts.append(doc.page_content)
            # metadata ä¿ç•™æ–‡ä»¶å + åŸå§‹ metadata
            meta = {"source": filename}
            if isinstance(doc.metadata, dict):
                meta.update(doc.metadata)
            batch_metas.append(meta)

            # è¾¾åˆ°æ‰¹é‡é˜ˆå€¼ â†’ è¿½åŠ å…¥åº“
            if len(batch_texts) >= batch_size:
                if vectorstore is None:
                    # é¦–æ‰¹ç”¨ from_texts åˆ›å»º
                    vectorstore = FAISS.from_texts(
                        batch_texts, embeddings, metadatas=batch_metas)
                else:
                    # åç»­æ‰¹æ¬¡è¿½åŠ 
                    vectorstore.add_texts(batch_texts, metadatas=batch_metas)

                processed += len(batch_texts)
                pbar.update(len(batch_texts))

                # æ¸…ç©ºæ‰¹ç¼“å­˜
                batch_texts, batch_metas = [], []

                # å‘¨æœŸæ€§ä¿å­˜
                if (processed // batch_size) % save_every_n_batches == 0:
                    vectorstore.save_local(INDEX_DIR)
                    pbar.set_postfix_str(f"ğŸ’¾ å·²ä¿å­˜è¿›åº¦ï¼š{processed}/{total_chunks}")

        # å•ä¸ªæ–‡ä»¶ç»“æŸæ—¶ä¹Ÿä¿å­˜ä¸€æ¬¡ï¼ˆç¨³å¦¥ï¼‰
        if vectorstore is not None:
            vectorstore.save_local(INDEX_DIR)

    # å¤„ç†æœ€åä¸è¶³ä¸€ä¸ª batch çš„æ®‹ä½™
    if batch_texts:
        if vectorstore is None:
            vectorstore = FAISS.from_texts(
                batch_texts, embeddings, metadatas=batch_metas)
        else:
            vectorstore.add_texts(batch_texts, metadatas=batch_metas)
        processed += len(batch_texts)
        pbar.update(len(batch_texts))
        batch_texts, batch_metas = [], []
        vectorstore.save_local(INDEX_DIR)

print(f"âœ… ç´¢å¼•å·²åˆ›å»ºå¹¶ä¿å­˜åˆ° {INDEX_DIR}ï¼ˆå…±å†™å…¥ {processed} ä¸ª chunkï¼‰")
