import os
from dotenv import load_dotenv
from langchain_openai import AzureChatOpenAI
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

# === åŠ è½½ç¯å¢ƒå˜é‡ ===
load_dotenv()

INDEX_DIR = "faiss_index"
embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-base")

# === åŠ è½½ç´¢å¼• ===
vectorstore = FAISS.load_local(
    INDEX_DIR, embeddings, allow_dangerous_deserialization=True)
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

# === LLMï¼ˆAzure OpenAIï¼‰ ===
llm = AzureChatOpenAI(
    azure_deployment=os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    openai_api_version=os.getenv("OPENAI_API_VERSION"),
    temperature=0.2
)

qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

print("ğŸ’¬ RAG ç³»ç»Ÿå·²å¯åŠ¨ï¼Œå¯ä»¥å¼€å§‹æé—®ã€‚è¾“å…¥ 'exit' é€€å‡ºã€‚\n")
while True:
    query = input("ğŸ§  è¯·è¾“å…¥ä½ çš„é—®é¢˜: ").strip()
    if query.lower() in ["exit", "quit"]:
        break
    result = qa.invoke(query)
    print("\nğŸ¤– ç­”æ¡ˆ:", result["result"])
    print("\nğŸ“– æ¥æºï¼š")
    for i, doc in enumerate(result["source_documents"], 1):
        print(f"--- ç‰‡æ®µ{i} --- æ¥è‡ª {doc.metadata.get('source')}")
        print(doc.page_content[:300], "...\n")
